{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ee411a-c902-426a-84f0-fd3a65d7a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65c2ab2-780b-41c9-85db-a8b22804cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[2.0,1.0,0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571db166-10b7-4b01-b643-fef92be96000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 1 [[0.65900114 0.24243297 0.09856589]]\n"
     ]
    }
   ],
   "source": [
    "tempreture = 1\n",
    "logits = torch.tensor(input /tempreture)\n",
    "softmax_score = F.softmax(logits, dim=1)\n",
    "print(f\"temperature = {tempreture} {softmax_score.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcdb475d-2bcd-4a9a-97c1-2de191070ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 0.1 [[9.99954597e-01 4.53978684e-05 5.60254205e-09]]\n"
     ]
    }
   ],
   "source": [
    "tempreture =0.1\n",
    "logits = torch.tensor(input /tempreture)\n",
    "softmax_score = F.softmax(logits, dim=1)\n",
    "print(f\"temperature = {tempreture} {softmax_score.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21950e9-2ca6-4f51-b0b7-90cc90fd8a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 20 [[0.34957672 0.33252767 0.31789561]]\n"
     ]
    }
   ],
   "source": [
    "tempreture = 20\n",
    "logits = torch.tensor(input /tempreture)\n",
    "softmax_score = F.softmax(logits, dim=1)\n",
    "print(f\"temperature = {tempreture} {softmax_score.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b07bcde-5dd5-48d7-8721-434fe06afe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用大模型的集中方法\n",
    "# 1. modelscope\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c0004db-27a5-49b0-bbba-a4a8cc920d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../model/Qwen2.5-0.5B-Instruct/'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "gen_config = GenerationConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d8ce40-422a-4771-b8c5-0bf8776802fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [108386, 3837, 546, 498, 10402, 30], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"你好，are you OK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa653562-b0b4-475b-8a64-2ef52cf58150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(108386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9254dd4a-d05c-4eaa-bf5f-1a101f5df3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d37de45-5fbc-4264-8cef-56efb3e3f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "----------------\n",
      "model_inputids: tensor([[151644,   8948,    198, 151645,    198, 151644,    872,    198,   9707,\n",
      "         151645,    198, 151644,  77091,    198]])\n",
      "generated_ids: tensor([[151644,   8948,    198, 151645,    198, 151644,    872,    198,   9707,\n",
      "         151645,    198, 151644,  77091,    198,   9707,      0,   2585,    646,\n",
      "            358,   7789,    498,   3351,     30, 151645]])\n",
      "response_ids: [tensor([  9707,      0,   2585,    646,    358,   7789,    498,   3351,     30,\n",
      "        151645])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_prompt(prompt, temperature =0.1, top_k=20, top_p=0.8, max_new_tokens=2048):\n",
    "    gen_config.temperature = temperature\n",
    "    gen_config.top_k =top_k\n",
    "    gen_config.top_p = top_p\n",
    "\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(text)\n",
    "    print(\"----------------\")\n",
    "    model_input = tokenizer([text], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(model_input.input_ids, max_new_tokens = max_new_tokens,generation_config=gen_config)\n",
    "    print(\"model_inputids:\",model_input.input_ids)\n",
    "    print(\"generated_ids:\", generated_ids)\n",
    "    response_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_input.input_ids, generated_ids)]\n",
    "    print(\"response_ids:\",response_ids)\n",
    "    response = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "prompt = \"Hello\"\n",
    "\n",
    "run_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aefc8f58-cc4f-4d28-90d3-703f910ff028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吗'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ed5a6c-1361-4284-a0b6-b8ae4d11a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ids = torch.tensor([[151644,   8948,    198, 151645,    198, 151644,    872,    198, 108386,\n",
    "         151645,    198, 151644,  77091,    198]])\n",
    "gen_ids = torch.tensor([[151644,   8948,    198, 151645,    198, 151644,    872,    198, 108386,\n",
    "         151645,    198, 151644,  77091,    198, 108386,   6313, 104139, 111728,\n",
    "         103929, 101037,  11319, 151645]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bddeba96-af0e-4cb1-a542-7477a4479ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "print(in_ids.shape)\n",
    "print(gen_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62efcece-401e-49aa-bf4a-ced5e7bc24ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= tensor([151644,   8948,    198, 151645,    198, 151644,    872,    198, 108386,\n",
      "        151645,    198, 151644,  77091,    198])\n",
      "torch.Size([14])\n",
      "14\n",
      "----------------------\n",
      "j= tensor([151644,   8948,    198, 151645,    198, 151644,    872,    198, 108386,\n",
      "        151645,    198, 151644,  77091,    198, 108386,   6313, 104139, 111728,\n",
      "        103929, 101037,  11319, 151645])\n",
      "torch.Size([22])\n",
      "tensor([151644,  77091,    198, 108386,   6313, 104139, 111728, 103929, 101037,\n",
      "         11319, 151645])\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(in_ids, gen_ids):\n",
    "    print(\"i=\",i)\n",
    "    print(i.shape)\n",
    "    print(len(i))\n",
    "    print(\"----------------------\")\n",
    "    print(\"j=\",j)\n",
    "    print(j.shape)\n",
    "    z = j[11:]\n",
    "    print(z)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92adda7a-475d-4146-ab06-d9a21aabb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2通过transformer调用模型\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "model_path = '../model/Qwen1.5-0.5B-Chat/'\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55b9159b-73bd-4707-b757-d496b59d8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "        (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0540c0b3-c5bc-48d5-bbaf-fc4cc96ab1e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (Qwen2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m     response, history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(tokenizer,\n\u001b[0;32m      3\u001b[0m                                   prompt,\n\u001b[0;32m      4\u001b[0m                                   history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m                                   top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                   max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m---> 11\u001b[0m chatglm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m, in \u001b[0;36mchatglm\u001b[1;34m(prompt, history, model, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatglm\u001b[39m(prompt, history\u001b[38;5;241m=\u001b[39m[], model \u001b[38;5;241m=\u001b[39m model, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer):\n\u001b[1;32m----> 2\u001b[0m     response, history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(tokenizer,\n\u001b[0;32m      3\u001b[0m                                   prompt,\n\u001b[0;32m      4\u001b[0m                                   history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m      5\u001b[0m                                   tempreture\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                   top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                   top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                   max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\bo\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\bo\\Lib\\site-packages\\transformers\\generation\\utils.py:2079\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \n\u001b[0;32m   1991\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 2079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[0;32m   2080\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   2081\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\bo\\Lib\\site-packages\\transformers\\generation\\utils.py:1363\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_generate():\n\u001b[0;32m   1357\u001b[0m     terminations_with_generation_support \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForConditionalGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForSpeechSeq2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForVision2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1362\u001b[0m     ]\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1364\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current model class (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with `.generate()`, as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a language model head. Classes that support generation often end in one of these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminations_with_generation_support\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (Qwen2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
     ]
    }
   ],
   "source": [
    "def chatglm(prompt, history=[], model = model, tokenizer=tokenizer):\n",
    "    response, history = model.generate(tokenizer,\n",
    "                                  prompt,\n",
    "                                  history=history,\n",
    "                                  tempreture=0.1,\n",
    "                                  top_p=0.8,\n",
    "                                  top_k=20,\n",
    "                                  max_length=8192)\n",
    "    return response\n",
    "\n",
    "chatglm(\"你好\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1ab74738-a589-4afb-845c-bea8db0ed75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a large language model created by Alibaba Cloud. I am called Qwen.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"../model/Qwen1.5-0.5B-Chat\")\n",
    "response = pipe(messages)\n",
    "response[0]['generated_text'][1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9feed95a-75b2-41d7-86dc-1672dce02f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../model//Qwen1.5-0.5B-Chat\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model//Qwen1.5-0.5B-Chat\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9f538c54-15dc-4154-8291-53c5d3190e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A large language model is a type of machine learning algorithm that can generate human-like text based on a given input, often in the form of a prompt or question. These models can be used for various purposes, such as language translation, text generation, sentiment analysis, and more. They have become increasingly important in recent years due to their ability to handle large amounts of data and quickly learn patterns and relationships within that data.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f36b116a-511c-4f6b-bddf-394e310c9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "541d8ab8-1193-4f52-9cb1-77d69d2d6359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.Stream object at 0x0000025FE2562C40>\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key = 'xx',\n",
    "    base_url='http://localhost:11434/v1'\n",
    ")\n",
    "reponse = client.chat.completions.create(\n",
    "    model = 'qwen2.5:3b',\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"\"},\n",
    "        {\"role\":\"user\",\"content\":\"你能做什么\"}\n",
    "    ],\n",
    "    temperature = 0.7,\n",
    "    stream = True\n",
    ")\n",
    "print (reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be4a38d8-000e-4bb8-9371-87657dc436aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reponse:\n",
    "    print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f08a3c-4f23-40a8-a491-2af77211950d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo_kernel",
   "language": "python",
   "name": "bo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
